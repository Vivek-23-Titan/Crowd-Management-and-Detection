{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Person_Detection_in_Crowd_YOLOv4.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPca0/NjgqkK1MJBS9Z7Vjz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vivek-23-Titan/Crowd-Management-and-Detection/blob/master/Person_Detection_in_Crowd_YOLOv4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jdwSplQfOOd",
        "colab_type": "text"
      },
      "source": [
        "# Clone the YOLOv4 github repo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QVbFchIZbORa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "a5f986fb-994a-4b68-ab85-cb15665bc998"
      },
      "source": [
        "!git clone https://github.com/Vivek-23-Titan/PyTorch_YOLOv4.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'PyTorch_YOLOv4'...\n",
            "remote: Enumerating objects: 30, done.\u001b[K\n",
            "remote: Counting objects:   3% (1/30)\u001b[K\rremote: Counting objects:   6% (2/30)\u001b[K\rremote: Counting objects:  10% (3/30)\u001b[K\rremote: Counting objects:  13% (4/30)\u001b[K\rremote: Counting objects:  16% (5/30)\u001b[K\rremote: Counting objects:  20% (6/30)\u001b[K\rremote: Counting objects:  23% (7/30)\u001b[K\rremote: Counting objects:  26% (8/30)\u001b[K\rremote: Counting objects:  30% (9/30)\u001b[K\rremote: Counting objects:  33% (10/30)\u001b[K\rremote: Counting objects:  36% (11/30)\u001b[K\rremote: Counting objects:  40% (12/30)\u001b[K\rremote: Counting objects:  43% (13/30)\u001b[K\rremote: Counting objects:  46% (14/30)\u001b[K\rremote: Counting objects:  50% (15/30)\u001b[K\rremote: Counting objects:  53% (16/30)\u001b[K\rremote: Counting objects:  56% (17/30)\u001b[K\rremote: Counting objects:  60% (18/30)\u001b[K\rremote: Counting objects:  63% (19/30)\u001b[K\rremote: Counting objects:  66% (20/30)\u001b[K\rremote: Counting objects:  70% (21/30)\u001b[K\rremote: Counting objects:  73% (22/30)\u001b[K\rremote: Counting objects:  76% (23/30)\u001b[K\rremote: Counting objects:  80% (24/30)\u001b[K\rremote: Counting objects:  83% (25/30)\u001b[K\rremote: Counting objects:  86% (26/30)\u001b[K\rremote: Counting objects:  90% (27/30)\u001b[K\rremote: Counting objects:  93% (28/30)\u001b[K\rremote: Counting objects:  96% (29/30)\u001b[K\rremote: Counting objects: 100% (30/30)\u001b[K\rremote: Counting objects: 100% (30/30), done.\u001b[K\n",
            "remote: Compressing objects:   3% (1/30)\u001b[K\rremote: Compressing objects:   6% (2/30)\u001b[K\rremote: Compressing objects:  10% (3/30)\u001b[K\rremote: Compressing objects:  13% (4/30)\u001b[K\rremote: Compressing objects:  16% (5/30)\u001b[K\rremote: Compressing objects:  20% (6/30)\u001b[K\rremote: Compressing objects:  23% (7/30)\u001b[K\rremote: Compressing objects:  26% (8/30)\u001b[K\rremote: Compressing objects:  30% (9/30)\u001b[K\rremote: Compressing objects:  33% (10/30)\u001b[K\rremote: Compressing objects:  36% (11/30)\u001b[K\rremote: Compressing objects:  40% (12/30)\u001b[K\rremote: Compressing objects:  43% (13/30)\u001b[K\rremote: Compressing objects:  46% (14/30)\u001b[K\rremote: Compressing objects:  50% (15/30)\u001b[K\rremote: Compressing objects:  53% (16/30)\u001b[K\rremote: Compressing objects:  56% (17/30)\u001b[K\rremote: Compressing objects:  60% (18/30)\u001b[K\rremote: Compressing objects:  63% (19/30)\u001b[K\rremote: Compressing objects:  66% (20/30)\u001b[K\rremote: Compressing objects:  70% (21/30)\u001b[K\rremote: Compressing objects:  73% (22/30)\u001b[K\rremote: Compressing objects:  76% (23/30)\u001b[K\rremote: Compressing objects:  80% (24/30)\u001b[K\rremote: Compressing objects:  83% (25/30)\u001b[K\rremote: Compressing objects:  86% (26/30)\u001b[K\rremote: Compressing objects:  90% (27/30)\u001b[K\rremote: Compressing objects:  93% (28/30)\u001b[K\rremote: Compressing objects:  96% (29/30)\u001b[K\rremote: Compressing objects: 100% (30/30)\u001b[K\rremote: Compressing objects: 100% (30/30), done.\u001b[K\n",
            "Receiving objects:   0% (1/156)   \rReceiving objects:   1% (2/156)   \rReceiving objects:   2% (4/156)   \rReceiving objects:   3% (5/156)   \rReceiving objects:   4% (7/156)   \rReceiving objects:   5% (8/156)   \rReceiving objects:   6% (10/156)   \rReceiving objects:   7% (11/156)   \rReceiving objects:   8% (13/156)   \rReceiving objects:   9% (15/156)   \rReceiving objects:  10% (16/156)   \rReceiving objects:  11% (18/156)   \rReceiving objects:  12% (19/156)   \rReceiving objects:  13% (21/156)   \rReceiving objects:  14% (22/156)   \rReceiving objects:  15% (24/156)   \rReceiving objects:  16% (25/156)   \rReceiving objects:  17% (27/156)   \rReceiving objects:  18% (29/156)   \rReceiving objects:  19% (30/156)   \rReceiving objects:  20% (32/156)   \rReceiving objects:  21% (33/156)   \rReceiving objects:  22% (35/156)   \rReceiving objects:  23% (36/156)   \rReceiving objects:  24% (38/156)   \rReceiving objects:  25% (39/156)   \rReceiving objects:  26% (41/156)   \rReceiving objects:  27% (43/156)   \rReceiving objects:  28% (44/156)   \rReceiving objects:  29% (46/156)   \rReceiving objects:  30% (47/156)   \rReceiving objects:  31% (49/156)   \rReceiving objects:  32% (50/156)   \rReceiving objects:  33% (52/156)   \rReceiving objects:  34% (54/156)   \rReceiving objects:  35% (55/156)   \rReceiving objects:  36% (57/156)   \rReceiving objects:  37% (58/156)   \rReceiving objects:  38% (60/156)   \rReceiving objects:  39% (61/156)   \rReceiving objects:  40% (63/156)   \rReceiving objects:  41% (64/156)   \rReceiving objects:  42% (66/156)   \rReceiving objects:  43% (68/156)   \rReceiving objects:  44% (69/156)   \rReceiving objects:  45% (71/156)   \rReceiving objects:  46% (72/156)   \rReceiving objects:  47% (74/156)   \rReceiving objects:  48% (75/156)   \rReceiving objects:  49% (77/156)   \rReceiving objects:  50% (78/156)   \rReceiving objects:  51% (80/156)   \rReceiving objects:  52% (82/156)   \rReceiving objects:  53% (83/156)   \rReceiving objects:  54% (85/156)   \rReceiving objects:  55% (86/156)   \rReceiving objects:  56% (88/156)   \rReceiving objects:  57% (89/156)   \rReceiving objects:  58% (91/156)   \rremote: Total 156 (delta 14), reused 1 (delta 0), pack-reused 126\u001b[K\n",
            "Receiving objects:  59% (93/156)   \rReceiving objects:  60% (94/156)   \rReceiving objects:  61% (96/156)   \rReceiving objects:  62% (97/156)   \rReceiving objects:  63% (99/156)   \rReceiving objects:  64% (100/156)   \rReceiving objects:  65% (102/156)   \rReceiving objects:  66% (103/156)   \rReceiving objects:  67% (105/156)   \rReceiving objects:  68% (107/156)   \rReceiving objects:  69% (108/156)   \rReceiving objects:  70% (110/156)   \rReceiving objects:  71% (111/156)   \rReceiving objects:  72% (113/156)   \rReceiving objects:  73% (114/156)   \rReceiving objects:  74% (116/156)   \rReceiving objects:  75% (117/156)   \rReceiving objects:  76% (119/156)   \rReceiving objects:  77% (121/156)   \rReceiving objects:  78% (122/156)   \rReceiving objects:  79% (124/156)   \rReceiving objects:  80% (125/156)   \rReceiving objects:  81% (127/156)   \rReceiving objects:  82% (128/156)   \rReceiving objects:  83% (130/156)   \rReceiving objects:  84% (132/156)   \rReceiving objects:  85% (133/156)   \rReceiving objects:  86% (135/156)   \rReceiving objects:  87% (136/156)   \rReceiving objects:  88% (138/156)   \rReceiving objects:  89% (139/156)   \rReceiving objects:  90% (141/156)   \rReceiving objects:  91% (142/156)   \rReceiving objects:  92% (144/156)   \rReceiving objects:  93% (146/156)   \rReceiving objects:  94% (147/156)   \rReceiving objects:  95% (149/156)   \rReceiving objects:  96% (150/156)   \rReceiving objects:  97% (152/156)   \rReceiving objects:  98% (153/156)   \rReceiving objects:  99% (155/156)   \rReceiving objects: 100% (156/156)   \rReceiving objects: 100% (156/156), 1.02 MiB | 2.37 MiB/s, done.\n",
            "Resolving deltas:   0% (0/56)   \rResolving deltas:   7% (4/56)   \rResolving deltas:   8% (5/56)   \rResolving deltas:  21% (12/56)   \rResolving deltas:  25% (14/56)   \rResolving deltas:  26% (15/56)   \rResolving deltas:  30% (17/56)   \rResolving deltas:  32% (18/56)   \rResolving deltas:  33% (19/56)   \rResolving deltas:  41% (23/56)   \rResolving deltas:  42% (24/56)   \rResolving deltas:  44% (25/56)   \rResolving deltas:  46% (26/56)   \rResolving deltas:  48% (27/56)   \rResolving deltas:  50% (28/56)   \rResolving deltas:  51% (29/56)   \rResolving deltas:  64% (36/56)   \rResolving deltas:  66% (37/56)   \rResolving deltas:  67% (38/56)   \rResolving deltas:  71% (40/56)   \rResolving deltas:  73% (41/56)   \rResolving deltas:  85% (48/56)   \rResolving deltas:  91% (51/56)   \rResolving deltas:  94% (53/56)   \rResolving deltas:  96% (54/56)   \rResolving deltas:  98% (55/56)   \rResolving deltas: 100% (56/56)   \rResolving deltas: 100% (56/56), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_Oi9x2dfUR_",
        "colab_type": "text"
      },
      "source": [
        "# Link Drive for dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vTHxrbbJcGJV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DYDFHaAKgVMo",
        "colab_type": "text"
      },
      "source": [
        "## Obtain the YOLOv4 weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DuXoD-d4bc5n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "8f56c9bf-245a-44b5-da8c-72514cb42490"
      },
      "source": [
        "#Copy the model weight file\n",
        "%cd /content/drive/My Drive/YOLOv4/\n",
        "%cp YOLOv4-pacsp.pt yolov4-pacsp.pt\n",
        "%ls\n",
        "\n",
        "#Move the copied YOLOv4 weight file\n",
        "!mv yolov4-pacsp.pt /content/PyTorch_YOLOv4/weights\n",
        "\n",
        "%cd /content/PyTorch_YOLOv4/weights\n",
        "%ls\n",
        "#yolov4-pacsp-x.pt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/YOLOv4\n",
            "'Copy of yolov4-pacsp.pt'         yolov4-pacsp.pt\n",
            "'Copy of yolov4-pacsp-x (1).pt'   YOLOv4-pacsp.pt\n",
            "'Copy of yolov4-pacsp-x.pt'       YOLOv4-pacsp-x.pt\n",
            "/content/PyTorch_YOLOv4/weights\n",
            "'put your weights file here.txt'   yolov4-pacsp.pt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KhmutpQzdHjv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "outputId": "25a339d1-7e89-415b-ed1c-8f15f75e04be"
      },
      "source": [
        "%cd /content/PyTorch_YOLOv4/\n",
        "!python3 detect.py #--weights weights/yolov4-pacsp-x.pt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/PyTorch_YOLOv4\n",
            "Namespace(agnostic_nms=False, augment=False, cfg='cfg/yolov4-pacsp.cfg', classes=None, conf_thres=0.3, device='', fourcc='mp4v', half=False, img_size=512, iou_thres=0.6, names='data/coco.names', output='output', save_txt=False, source='data/samples', view_img=False, weights='weights/yolov4-pacsp.pt')\n",
            "Using CUDA device0 _CudaDeviceProperties(name='Tesla K80', total_memory=11441MB)\n",
            "\n",
            "Model Summary: 342 layers, 5.29214e+07 parameters, 5.29214e+07 gradients\n",
            "image 1/2 data/samples/bus.jpg: 512x384 3 persons, 1 buss, 1 ties, Done. (0.084s)\n",
            "image 2/2 data/samples/zidane.jpg: 320x512 2 persons, 3 ties, Done. (0.082s)\n",
            "Results saved to /content/PyTorch_YOLOv4/output\n",
            "Done. (0.717s)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FWZD6w6pgYxz",
        "colab_type": "text"
      },
      "source": [
        "## Obtain the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Ork1dNckQti",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "009c1836-17c8-4a0b-f373-3f0ba9c9dd02"
      },
      "source": [
        "#Copy the Dataset zip file\n",
        "%cd /content/drive/My Drive/Crowd_Control/Dataset and Paper/\n",
        "%cp Copy_of_Five_ClassDataset.zip Five_ClassDataset.zip\n",
        "%ls\n",
        "\n",
        "#Move the copied dataset zip file\n",
        "!mv Five_ClassDataset.zip /content/PyTorch_YOLOv4/data/samples\n",
        "\n",
        "%cd /content/PyTorch_YOLOv4/data/samples\n",
        "%ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/.shortcut-targets-by-id/1txhspqlWXxVHAc3K5WpgpvGwogLIEY-q/Dataset and Paper\n",
            " 39.pf.doc\n",
            "'Copy of Five_ClassDataset.zip'\n",
            " Copy_of_Five_ClassDataset.zip\n",
            "\u001b[0m\u001b[01;34m'Copy_of_Five_ClassDataset.zip (Unzipped Files)'\u001b[0m/\n",
            " Five_ClassDataset.zip\n",
            "/content/PyTorch_YOLOv4/data/samples\n",
            "bus.jpg  Five_ClassDataset.zip  zidane.jpg\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tKxg2iP9kYeq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "4763dd42-1be1-412e-c72d-786475465fb7"
      },
      "source": [
        "from zipfile import ZipFile\n",
        "file_name = '/content/PyTorch_YOLOv4/data/samples/Five_ClassDataset.zip'\n",
        "\n",
        "with ZipFile(file_name, 'r') as zip:\n",
        "  zip.extractall()\n",
        "  print('done')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1yxCohNmkyzd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "b983392d-2042-496c-9ccd-02ec6f1ef6c6"
      },
      "source": [
        "#Copy the data to samples folder\n",
        "%cd /content/PyTorch_YOLOv4/data/samples/Dataset/Test/Test_VeryHigh/\n",
        "%cp * /content/PyTorch_YOLOv4/data/samples\n",
        "\n",
        "%cd /content/PyTorch_YOLOv4/data/samples/\n",
        "#%ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/PyTorch_YOLOv4/data/samples/Dataset/Test/Test_VeryHigh\n",
            "/content/PyTorch_YOLOv4/data/samples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GTNqhvcxfwcR",
        "colab_type": "text"
      },
      "source": [
        "# Run YOLOv4 for object detection based on the COCO classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rF0aPRkItXtE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%cd /content/PyTorch_YOLOv4/\n",
        "!python3 detect.py --conf-thres 0.2 #--iou-thres 0.5 #--conf-thres 0.2 --weights weights/yolov4-pacsp-x.pt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "swfjRz9RlOwK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Clear the data and results if required to run again\n",
        "!rm -rf /content/PyTorch_YOLOv4/output/*.jpg\n",
        "!rm -rf /content/PyTorch_YOLOv4/data/samples/*.jpg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HnpXCStNgJ15",
        "colab_type": "text"
      },
      "source": [
        "# Store and the display the results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q1-iSi6ll1cU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "66d71fa6-ad83-4653-d402-4e1b007e7676"
      },
      "source": [
        "#Storing the video frames into an array\n",
        "import glob\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "path = glob.glob(r\"/content/PyTorch_YOLOv4/output/*.jpg\")\n",
        "pic = []\n",
        "\n",
        "for img in path:\n",
        "    n = cv2.imread(img)\n",
        "    n = cv2.resize(n, (512, 256))\n",
        "    pic.append(n)\n",
        "\n",
        "pic = np.array(pic)\n",
        "print(pic.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(300, 256, 512, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pOnmw03lmgI4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Bounding Box on Masked faces from the video frames\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "for i in range(len(pic)):\n",
        "\n",
        "  # load the photo and extract the face\n",
        "  try:\n",
        "      plt.rcParams[\"figure.figsize\"] = (6,4)\n",
        "      plt.imshow(cv2.cvtColor(pic[i], cv2.COLOR_BGR2RGB))\n",
        "      plt.axis(\"off\")\n",
        "      plt.show()\n",
        "      print(\"Frame:\", i)\n",
        "  except:\n",
        "    continue"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p20rhhTPgE8c",
        "colab_type": "text"
      },
      "source": [
        "# Load and Open the Text File"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6X2jf82FfVq2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "f = open('/content/VeryHigh_0.2.txt','r')\n",
        "#print(f.read(49))\n",
        "line = []\n",
        "for lines in f:\n",
        "  #line = f.readline()\n",
        "  try:\n",
        "    a = lines.index('persons')\n",
        "    line.append(lines[a-3:a-1].lstrip())\n",
        "  except:\n",
        "    continue\n",
        "f.close()\n",
        "#line"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h2jUatFEh7Q6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "k5 = [int(j) for j in line]\n",
        "print(sum(k)/len(k))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IjZvXms081Nw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "619ecfa4-699c-47bf-8d34-683f8442bedd"
      },
      "source": [
        "d = {}\n",
        "for key in k:\n",
        "  d[key] = d.get(key, 0) + 1\n",
        "print(sorted(d.items()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(1, 189), (2, 17), (3, 13), (4, 38), (5, 14), (6, 1)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uD9erk3hgA3N",
        "colab_type": "text"
      },
      "source": [
        "## Box Plot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qOS5IKh5QmBA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 268
        },
        "outputId": "0d98c307-1335-44bf-b5b0-70f57be94920"
      },
      "source": [
        "import seaborn as sns\n",
        "sns.set(style=\"whitegrid\")\n",
        "#ax = sns.boxplot([k1, k2, k3, k4, k5])\n",
        "ax = sns.boxplot(data=([k1, k2, k3, k4, k5]))\n",
        "#ax = sns.boxplot(x=\"day\", y=\"total_bill\", data=({'VeryLow':k1, 'Low':k2, 'Moderate':k3, 'High':k4, 'VeryHigh':k5}))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD7CAYAAAB68m/qAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAU6UlEQVR4nO3db2yT16HH8Z/t4BQyIDEbxYXQNmjpoqKraonaN2sFDoiKpX/eJcqqSWxlKmh9sS7d6IZIR2E0bVZt1eiY2HTfFNXVuBJ3YYg0JKCrTlq1sVVXTlrCxSmpRkIb7KwhQIJj3xdZUgKhyeN/xz7+fqS+sKPH5xc3/uVw8pzncSUSiYQAAHnPbToAACA9KHQAsASFDgCWoNABwBIUOgBYosjUwPF4XKOjo1qwYIFcLpepGACQVxKJhK5fv66SkhK53TPn5MYKfXR0VL29vaaGB4C8VllZqcWLF894zlihL1iwQNJkKK/XayoGAOSV8fFx9fb2TnfojYwV+tQyi9frVXFxsakYAJCXZluq5o+iAGAJCh0ALEGhA4AlKHQAtxUOh1VfX6++vj7TUTAPFDqA22ptbdWVK1fU2tpqOgrmgUIHMKtwOKyPP/5YktTf388sPQ9Q6ABmdfOsnFl67qPQAcxqanY+pb+/31ASzBeFDmBW5eXlMx6vXr3aUBLMF4UOYFZNTU1f+Bi5h0IHMKuKiorpWfrq1at17733Gk6EuVDoAG6rqalJixYtYnaeJ4xdnAtA7quoqNDbb79tOgbmiRk6AFiCQgcAS7DkAhSIrq4udXR0ODpmeHhYklRaWurouI0bNyoQCDg6Bqmj0AHcViQSkeS80GEGhQ4UiEAg4HjW/MILL0iS9u3bl4lISDPW0AHAEhQ6AFiCQgcAS1DoAGAJCh0ALEGhA4AlKHQAsASFDgCWoNABwBIUOgBYgkIHAEtQ6ABgiTkvzhWNRvWjH/1I/f398nq9uvvuu7V79275fD69//772rVrl8bGxrRy5Uq9+uqrWrZsWTZyAwBuMucM3eVy6emnn1Z7e7va2tpUXl6u1tZWxeNxPf/889q1a5fa29tVU1Oj1tbWbGQGAMxizkIvLS3VQw89NP34gQce0IULFxQKhVRcXKyamhpJUkNDg44fP565pACAL+ToeujxeFxvvfWWAoGABgYGdNddd01/zefzKR6Pa3h4mIvhI2dwlx4UEkeF/tJLL2nRokV66qmnHH9IbicUCqXldYDZ9PX1aWRkxNExn376qSTJ4/E4Huv06dOOjsl1U++dbd+XreZd6C0tLTp//rwOHDggt9stv9+vCxcuTH89EonI7XY7ntWsXbtWxcXFjo4B5qu6utrxMdyl53OHDx+WlNz7iMwYGxu77UR4XqctvvbaawqFQtq/f7+8Xq+kySK+du2a/va3v0mSgsGgHn300TRFBgA4NecM/ezZs/rtb3+re+65Rw0NDZKkVatWaf/+/XrllVfU3Nw847RFAIAZcxb6V7/6VZ05c2bWr339619XW1tb2kMBAJxjpygAWIJCBwBLUOgAYAkKHQAsQaEDgCUodACwBIUOAJag0AHAEhQ6AFiCQgcAS1DoAGAJCh0ALOHoBhcAzDt48KDC4XBWxpoaZ+oa8ZlWUVGhrVu3ZnwcW+9kRaEDeSYcDqu3u1tfdnhHpWR443FJUuTDDzM+1tDERMbHSEUkEpHkvNCziUIH8tCXPR49sTh3iyUZ/z0ynLWxAoGA41lzPtzJijV0ALAEhQ4AlqDQAcASFDoAWIJCBwBLUOgAYAlOW0ReYDMNMDcKHXkhHA6r+0yPPEu9GR8r7p7c4PLh4P9lfKyJf41nfAwUDgodecOz1Kulj9xlOkZa/et/LpiOAIuwhg4AlqDQAcASFDoAWIJCh6TJK8nt2LFD0WjUdBQASaLQIUkKBoPq6elRMBg0HQVAkih0KBKJqLOzU4lEQidOnGCWDuQpTluEgsGg4v++kUE8HlcwGNS2bdsMp5opGo0qNjxm3Wl+seExRYud/QKNRqMaisWyev3wbBiKxeRiMpESZujQqVOnFIvFJEmxWEwnT540nAhAMpihQ+vWrVNHR4disZiKioq0fv1605FuUVZWpotjl6zcWFRWVubomLKyMiUuXrTyjkVO3wvMxAwdamhokNs9+aPgdrvV0NBgOBGAZFDokM/nU21trVwulzZs2MAsCchTLLlA0uQsvb+/n9k5kMfmVegtLS1qb2/XP//5T7W1tamyslLS5J2zvV6viouLJUlNTU16+OGHM5cWAHBb8yr02tpaffvb39a3vvWtW772+uuvTxc88teNG4ty7ZRFAPMzrzX0mpoa+f3+TGeBIWwsAuyQ8hp6U1OTEomEqqur9dxzz2nJkiXpyIUsyoeNRdLkzSCysbEofm3yBhfuOzwZH2viX+PSCufHDU1MZGVj0ZV//1wscmf+/ImhiQn5Mj6K3VIq9EOHDsnv92t8fFx79+7V7t271dra6ug1QqFQKhGQBl1dXTM2FnV2durBBx80nGqmkpISrb6rPCtjDY4MSpJWLLsz84OVTH5vp0+fnv8hJSXyr16dwVCfuzo4+V4sXZHEbx2H/HL+XmTTyMiIJOVsPinFQp9ahvF6vWpsbExqVrd27drpP6rCjEAgMGNjUW1traqrq03HmiGbeabuJbpv376sjekE74UZhw8flpTd9382Y2Njt50IJ/3vqCtXrkz/xkokEjp27JiqqqqSfTkYxMYiwA7zmqHv2bNH77zzjoaGhrRlyxaVlpbqwIEDevbZZzUxMaF4PK41a9aoubk503mRAVMbi44fP87GIiCPzavQd+7cqZ07d97y/JEjR9IeCGawsQjIf+wUhaTJWfrLL79sOgaAFHAtFwCwBIUOAJZgycVCXV1d6ujocHTM8PDkJpXSUufX2N64caMCgYDj4wCkF4UOSZPb/6XkCh1AbqDQLRQIBBzPmNlAAuQ/1tABwBIUOgBYgkIHAEtQ6ABgCQodACxBoQOAJThtEUDeOnjwoMLhcFbGmhpn6hTfTKuoqNDWrVsdHUOhA8hb4XBYH/ScUcnCzN+8Lh6brMv+vk8zPtbo1UhSx1HoAPJayUKf7l/zqOkYadV97nhSx7GGDgCWoNABwBIUOgBYgkIHAEtQ6ABgCQodACzBaYuwWjJ3b0p2Awl3boJpFDpwE58v85tUgEyg0GG1ZO7eBOQr1tABwBIUOgBYgkIHAEtQ6MBNIpGIduzYoWg0ajoK4AiFDtwkGAyqp6dHwWDQdBTAEQoduEEkElFnZ6cSiYROnDjBLB15hdMWcxh3Y8m+YDCoeDwuSYrH4woGg9q2bZvhVOlh4yaraDSq0auRpK8fnqtGr0YUjTqvZwo9h4XDYZ39oFsrvpT5/00LE5MlNvLxmYyPNXg5lvExknXq1CnFYpP5YrGYTp48aU2hJ4NNVvmFQs9xK75UpC3/YdeH6j//N7nba2XDunXr1NHRoVgspqKiIq1fv950pLSxcZNVWVmZRoZjVt6xqKyszPFxrKEDN2hoaJDbPfmxcLvdamhoMJwImD8KHbiBz+dTbW2tXC6XNmzYkNQsCTCFJRfgJg0NDerv72d2jrwz5wy9paVFgUBA9913n3p7e6ef7+vrU319vTZt2qT6+np99NFHmcwJZM1f/vIXdXd367333jMdxTg2WeWXOQu9trZWhw4d0sqVK2c839zcrMbGRrW3t6uxsVG7du3KWEggmw4cOCBJeuONNwwnMY9NVvllzkKvqamR3++f8dylS5fU09Ojuro6SVJdXZ16enoUieTu2QvAfBw7dkyJREKSlEgkdPy4Xec3O8Emq/yT1Br6wMCA7rzzTnk8HkmSx+PR8uXLNTAwwHmraRSNRjV0OZbTp/klY/ByTLEcLYep2fmUN954Q48+atcpcfNl8yYrWxn/o2goFDIdIWddu3bNdISMuXbtmk6fPm06xi2mZuc3Ps7FnNnQ1dU1Y5NVZ2enHnzwQcOpZhoZGTEdIWNGRkYc/+wlVeh+v18XL17UxMSEPB6PJiYm9Mknn9yyNDMfa9euVXFxcTIxrOf3+zUS+8zKjUWL/X5VV1ebjnILl8s1o9RdLldO5syGQCAwY5NVbW1tzr0Xhw8fVnTIzonP4sWLZ32/x8bGbjsRTuo89GXLlqmqqkpHjx6VJB09elRVVVUstyDvPfPMMzMeb9++3VAS89hklX/mLPQ9e/bokUce0eDgoLZs2aJvfvObkqQXX3xRb775pjZt2qQ333xTP/vZzzIeFsi0zZs3y+VySZqcnRfq+rnEJqt8NOeSy86dO7Vz585bnl+zZo3+8Ic/ZCQUYNIzzzyj3/zmNwU9O5/CJqv8YvyPokCu2bx5szZv3mw6BuAY13IBcFtsLMovFDqAWbGxKP+w5JLjBrO0sejy+OQGki95M/87fvByTIszPgpSlS8bi7J1x6Lx61clSd4FCzM+1ujViKSvOD6OQs9hFRUVWRvr03/fasxfnvkxFyu73xuSkw93b8rmz9HU7fhW3+u8aJ37SlLfG4Wew7J5z82pe0bu27cva2Mit+XD3Zv4jMzEGjqAWbGxKP9Q6ABmxcai/MOSC4DbYmNRfqHQAdyWz+fTyy+/bDoG5oklFwCwBIUOAJag0AHAEhQ6AFiCQgcAS1DoAGAJCh0ALEGhA4AlKHQAsASFDgCWoNABwBIUOgBYgkIHAEtQ6ABgCQodACxBoQOAJQq60CORiHbs2KFoNGo6CgCkrKALPRgMqqenR8Fg0HQUAEhZwRZ6JBJRZ2enEomETpw4wSwdQN4r2HuKBoNBxeNxSVI8HlcwGNS2bdsMp0qPrq4udXR0ODomHA5Lkl544QXH423cuFGBQMDxcQDSq2Bn6KdOnVIsFpMkxWIxnTx50nAis3w+n3w+n+kYAFJQsDP0devWqaOjQ7FYTEVFRVq/fr3pSGkTCASYMQMFqGBn6A0NDXK7J799t9uthoYGw4kAIDUFW+g+n0+1tbVyuVzasGGDysrKTEcCgJQU7JKLNDlL7+/vZ3YOwAoFO0OXpL1796q7u1s///nPTUcBgJSlPEMPBALyer0qLi6WJDU1Nenhhx9OOVg29Pb2SpI+/PBDw0kAIHVpWXJ5/fXXVVlZmY6Xypof/vCHMx4///zzevXVVw2lAYDUFewa+tTsfAqzdKBwZHPzXTY33qWl0JuampRIJFRdXa3nnntOS5YsmfexoVAoHRHS4vTp06YjAMiCvr4+jYyMODpm4cKFkuT4uL6+vqx1iyuRSCRSeYGBgQH5/X6Nj49r7969Gh0dVWtr65zHjY2NKRQKae3atdPr79n02GOP3fJcW1tb1nMAgBNf1J0pn+Xi9/slSV6vV42Njfr73/+e6ktmxc1r/l/72tcMJQGA9Eip0K9cuTL9z49EIqFjx46pqqoqLcEy7Re/+MWMx/xBFEC+S2kN/dKlS3r22Wc1MTGheDyuNWvWqLm5OV3ZMq6yslK9vb3MzgFYIaVCLy8v15EjR9KVJes4Dx2ATQp6pygA2KRgC/3ms1xmO+sFAPJJwRY6ANiGQgcAS1DoAGAJCh0ALFGwhX7zNn+2/QPIdwVb6ABgm4K9fK7ErByAXZihA4AlKHQAsIQ1Sy7J3IFkeHhYklRaWup4vGzehQQA5sOaQk9GJBKRlFyhA0CusabQA4GA4xnz1L0B9+3bl4lIAJBVrKEDgCUodACwBIUOAJag0AHAEhQ6AFiCQgcAS+TcaYsHDx5UOBzOylhT40ydvphpFRUV2rp1a1bGAlB4cq7Qw+GwQj1n5Lkj85t94jGPJOmD8MWMjzVxbTjjYwAobDlX6JLkuaNUi+6uNR0jra6c7zQdAYDlWEMHAEtQ6ABgCQodACxBoQOAJSh0ALAEhQ4Alsi50xaj0agmrgxp5Mx/OTswEZ/8L1tc7sn/5iseUzTqzVweAAUv5wp9+fLlikajjo+LxWKKxRIZSDS7oiKPioqcvH0LtHz58ozlAYCcK/Tdu3ebjgAAeYk1dACwBIUOAJag0AHAEhQ6AFgi5ULv6+tTfX29Nm3apPr6en300UdpiAUAcCrlQm9ublZjY6Pa29vV2NioXbt2pSMXAMChlAr90qVL6unpUV1dnSSprq5OPT09ikQiaQkHAJi/lM5DHxgY0J133imPZ/LOPx6PR8uXL9fAwIB8Pt8XHptITG4CCoVCqUQAgII01aE3MvZH0evXr5saGgDy3mwdmtIM3e/36+LFi5qYmJDH49HExIQ++eQT+f3+OY8tKSlRZWWlFixYIJfLlUoMACgYiURC169fV0lJyS1fS6nQly1bpqqqKh09elRPPPGEjh49qqqqqjmXWyTJ7XZr8eLFqQwPAAXpjjvumPV5V2K2hRgHzp07px07duizzz7TkiVL1NLSooqKilReEgCQhJQLHQCQG9gpCgCWoNABwBIUOgBYgkIHAEtQ6ABgiYItdK4S+bmWlhYFAgHdd9996u3tNR3HmGg0qq1bt2rTpk167LHH9P3vf7+gr0u0fft2Pf7443ryySfV2NioDz74wHQk437961/n9OekYAudq0R+rra2VocOHdLKlStNRzHK5XLp6aefVnt7u9ra2lReXq7W1lbTsYxpaWnRH//4Rx05ckTf+c539JOf/MR0JKO6u7v1/vvv5/TnpCALnatEzlRTUzOvyzXYrrS0VA899ND04wceeEAXLlwwmMisG3dyX758uaAv0TE+Pq7du3frxRdfNB3lC6W09T9fpXKVSBSGeDyut956S4FAwHQUo37605/qz3/+sxKJhH73u9+ZjmPMr371Kz3++ONatWqV6ShfqCBn6MBcXnrpJS1atEhPPfWU6ShG7d27V6dOndIPfvADvfLKK6bjGPGPf/xDoVBIjY2NpqPMqSAL/carREpydJVI2K+lpUXnz5/XL3/5S7ndBfkRucWTTz6p9957T9Fo1HSUrPvrX/+qc+fOqba2VoFAQIODg/rud7+rd99913S0WxTkT+uNV4mU5OgqkbDba6+9plAopP3798vr9ZqOY8zo6KgGBgamH3d1dWnp0qUqLS01mMqM733ve3r33XfV1dWlrq4urVixQr///e/1jW98w3S0WxTsxbm4SuTn9uzZo3feeUdDQ0MqKytTaWmp/vSnP5mOlXVnz55VXV2d7rnnnunLk65atUr79+83nCz7hoaGtH37dl29elVut1tLly7Vj3/8Y91///2moxkXCAR04MABVVZWmo5yi4ItdACwTUEuuQCAjSh0ALAEhQ4AlqDQAcASFDoAWIJCBwBLUOgAYAkKHQAs8f/r9tKhG1/gowAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}
